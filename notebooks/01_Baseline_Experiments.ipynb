{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e31dfab5",
   "metadata": {},
   "source": [
    "### Installing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd6b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/One-sixth/fairseq.git sacrebleu tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7748429a",
   "metadata": {},
   "source": [
    "### Pipeline A (Baseline, 10k BPE Operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a501149a",
   "metadata": {},
   "source": [
    "#### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fde0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/NLP_TRANSLATION\n",
    "!mkdir /content/drive/MyDrive/NLP_TRANSLATION/bin\n",
    "!mkdir /content/drive/MyDrive/NLP_TRANSLATION/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b95c03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-preprocess \\\n",
    "    --source-lang fi --target-lang en \\\n",
    "    --trainpref /content/drive/MyDrive/NLP_TRANSLATION/pipeline_A_10k/data/train.bpe \\\n",
    "    --validpref /content/drive/MyDrive/NLP_TRANSLATION/pipeline_A_10k/data/dev.bpe \\\n",
    "    --testpref /content/drive/MyDrive/NLP_TRANSLATION/pipeline_A_10k/data/test.bpe \\\n",
    "    --destdir /content/drive/MyDrive/NLP_TRANSLATION/pipeline_A_10k/bin \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d2099",
   "metadata": {},
   "source": [
    "#### Training Fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f7b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-train /content/drive/MyDrive/NLP_TRANSLATION/pipeline_A_10k/bin \\\n",
    "    --arch transformer_iwslt_de_en \\\n",
    "    --share-decoder-input-output-embed \\\n",
    "    --source-lang fi --target-lang en \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 8192 \\\n",
    "    --update-freq 1 \\\n",
    "    --eval-bleu \\\n",
    "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
    "    --eval-bleu-detok space \\\n",
    "    --eval-bleu-remove-bpe \\\n",
    "    --eval-bleu-print-samples \\\n",
    "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
    "    --save-dir /content/drive/MyDrive/NLP_TRANSLATION/pipeline_A_10k/checkpoints \\\n",
    "    --max-epoch 30 \\\n",
    "    --patience 10 \\\n",
    "    --fp16 \\\n",
    "    --no-epoch-checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b658ec",
   "metadata": {},
   "source": [
    "#### Testing Fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceab266",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-generate /content/drive/MyDrive/NLP_TRANSLATION/pipeline_A_10k/bin \\\n",
    "    --path /content/drive/MyDrive/NLP_TRANSLATION/pipeline_A_10k/checkpoints/checkpoint_best.pt \\\n",
    "    --batch-size 128 \\\n",
    "    --beam 5 \\\n",
    "    --remove-bpe \\\n",
    "    > /content/drive/MyDrive/NLP_TRANSLATION/pipeline_A_10k/results_A_10k.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8533b0",
   "metadata": {},
   "source": [
    "#### Evaluation (SacreBLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16860226",
   "metadata": {},
   "source": [
    "> _NOTE:_  Be sure you are under the Project Root Folder (`NLP_TRANSLATION`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aa5159",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "moses_scripts=\"tools/mosesdecoder/scripts\"\n",
    "\n",
    "input_file=\"pipeline_A_10k/results_A_10k.txt\"\n",
    "\n",
    "original_ref_file=\"data/test/test.en\"\n",
    "\n",
    "# temporary files\n",
    "src_tok=\"pipeline_A_10k/source.tok\"\n",
    "hypo_tok=\"pipeline_A_10k/hypothesis.tok\"\n",
    "ref_tok=\"pipeline_A_10k/reference.tok\"\n",
    "\n",
    "src_dtc=\"pipeline_A_10k/source.dtc\"\n",
    "hypo_dtc=\"pipeline_A_10k/hypothesis.dtc\"\n",
    "ref_dtc=\"pipeline_A_10k/reference.dtc\"\n",
    "\n",
    "# output files\n",
    "src_detok=\"pipeline_A_10k/source.detok\"\n",
    "hypo_detok=\"pipeline_A_10k/hypothesis.detok\"\n",
    "ref_detok=\"pipeline_A_10k/reference.detok\"\n",
    "\n",
    "# extract detokenized texts\n",
    "grep ^S- \"$input_file\" | sed 's/^S-//' | sort -n | cut -f2- > \"$src_tok\"\n",
    "grep ^H- \"$input_file\" | sed 's/^H-//' | sort -n | cut -f3- > \"$hypo_tok\" # hypothesis (translated text)\n",
    "grep ^T- \"$input_file\" | sed 's/^T-//' | sort -n | cut -f2- > \"$ref_tok\" # refernece (gold standard text)\n",
    "\n",
    "# detruecasing\n",
    "perl \"$moses_scripts/recaser/detruecase.perl\" < \"$src_tok\" > \"$src_dtc\"\n",
    "perl \"$moses_scripts/recaser/detruecase.perl\" < \"$hypo_tok\" > \"$hypo_dtc\"\n",
    "perl \"$moses_scripts/recaser/detruecase.perl\" < \"$ref_tok\" > \"$ref_dtc\"\n",
    "\n",
    "# detokenizing\n",
    "perl \"$moses_scripts/tokenizer/detokenizer.perl\" -l fi < \"$src_dtc\" > \"$src_detok\"\n",
    "perl \"$moses_scripts/tokenizer/detokenizer.perl\" -l en < \"$hypo_dtc\" > \"$hypo_detok\"\n",
    "perl \"$moses_scripts/tokenizer/detokenizer.perl\" -l en < \"$ref_dtc\" > \"$ref_detok\"\n",
    "\n",
    "score=$(sacrebleu \"$original_ref_file\" -i \"$hypo_detok\" -m bleu -b -w 4)\n",
    "\n",
    "# cleanup\n",
    "rm \"$src_tok\" \"$hypo_tok\" \"$ref_tok\" \"$src_dtc\" \"$hypo_dtc\" \"$ref_dtc\"\n",
    "echo \"Pipeline A (10k) BLEU SCORE: $score\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87c3ee8",
   "metadata": {},
   "source": [
    "### Pipeline A (Baseline, 20k BPE Operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac6f4d3",
   "metadata": {},
   "source": [
    "#### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb40266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/NLP_TRANSLATION\n",
    "!mkdir /content/drive/MyDrive/NLP_TRANSLATION/bin\n",
    "!mkdir /content/drive/MyDrive/NLP_TRANSLATION/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef6dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-preprocess \\\n",
    "    --source-lang fi --target-lang en \\\n",
    "    --trainpref /content/drive/MyDrive/NLP_TRANSLATION/pipeline_A_20k/data/train.bpe \\\n",
    "    --validpref /content/drive/MyDrive/NLP_TRANSLATION/pipeline_A_20k/data/dev.bpe \\\n",
    "    --testpref /content/drive/MyDrive/NLP_TRANSLATION/pipeline_A_20k/data/test.bpe \\\n",
    "    --destdir /content/drive/MyDrive/NLP_TRANSLATION/pipeline_A_20k/bin \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ec9094",
   "metadata": {},
   "source": [
    "#### Training Fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183cc08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-train /content/drive/MyDrive/NLP_TRANSLATION/pipeline_A_20k/bin \\\n",
    "    --arch transformer_iwslt_de_en \\\n",
    "    --share-decoder-input-output-embed \\\n",
    "    --source-lang fi --target-lang en \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 8192 \\\n",
    "    --update-freq 1 \\\n",
    "    --eval-bleu \\\n",
    "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
    "    --eval-bleu-detok space \\\n",
    "    --eval-bleu-remove-bpe \\\n",
    "    --eval-bleu-print-samples \\\n",
    "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
    "    --save-dir /content/drive/MyDrive/NLP_TRANSLATION/pipeline_A_20k/checkpoints \\\n",
    "    --max-epoch 30 \\\n",
    "    --patience 10 \\\n",
    "    --fp16 \\\n",
    "    --no-epoch-checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be06672",
   "metadata": {},
   "source": [
    "#### Testing Fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-generate /content/drive/MyDrive/NLP_TRANSLATION/pipeline_A_20k/bin \\\n",
    "    --path /content/drive/MyDrive/NLP_TRANSLATION/pipeline_A_20k/checkpoints/checkpoint_best.pt \\\n",
    "    --batch-size 128 \\\n",
    "    --beam 5 \\\n",
    "    --remove-bpe \\\n",
    "    > /content/drive/MyDrive/NLP_TRANSLATION/pipeline_A_20k/results_A_20k.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d0496",
   "metadata": {},
   "source": [
    "#### Evaluation (SacreBLEU)\n",
    "\n",
    "> _NOTE:_  Be sure you are under the Project Root Folder (`NLP_TRANSLATION`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef89bf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "moses_scripts=\"tools/mosesdecoder/scripts\"\n",
    "\n",
    "input_file=\"pipeline_A_20k/results_A_20k.txt\"\n",
    "\n",
    "original_ref_file=\"data/test/test.en\"\n",
    "\n",
    "# temporary files\n",
    "src_tok=\"pipeline_A_20k/source.tok\"\n",
    "hypo_tok=\"pipeline_A_20k/hypothesis.tok\"\n",
    "ref_tok=\"pipeline_A_20k/reference.tok\"\n",
    "\n",
    "src_dtc=\"pipeline_A_20k/source.dtc\"\n",
    "hypo_dtc=\"pipeline_A_20k/hypothesis.dtc\"\n",
    "ref_dtc=\"pipeline_A_20k/reference.dtc\"\n",
    "\n",
    "# output files\n",
    "src_detok=\"pipeline_A_20k/source.detok\"\n",
    "hypo_detok=\"pipeline_A_20k/hypothesis.detok\"\n",
    "ref_detok=\"pipeline_A_20k/reference.detok\"\n",
    "\n",
    "# extract detokenized texts\n",
    "grep ^S- \"$input_file\" | sed 's/^S-//' | sort -n | cut -f2- > \"$src_tok\"\n",
    "grep ^H- \"$input_file\" | sed 's/^H-//' | sort -n | cut -f3- > \"$hypo_tok\" # hypothesis (translated text)\n",
    "grep ^T- \"$input_file\" | sed 's/^T-//' | sort -n | cut -f2- > \"$ref_tok\" # refernece (gold standard text)\n",
    "\n",
    "# detruecasing\n",
    "perl \"$moses_scripts/recaser/detruecase.perl\" < \"$src_tok\" > \"$src_dtc\"\n",
    "perl \"$moses_scripts/recaser/detruecase.perl\" < \"$hypo_tok\" > \"$hypo_dtc\"\n",
    "perl \"$moses_scripts/recaser/detruecase.perl\" < \"$ref_tok\" > \"$ref_dtc\"\n",
    "\n",
    "# detokenizing\n",
    "perl \"$moses_scripts/tokenizer/detokenizer.perl\" -l fi < \"$src_dtc\" > \"$src_detok\"\n",
    "perl \"$moses_scripts/tokenizer/detokenizer.perl\" -l en < \"$hypo_dtc\" > \"$hypo_detok\"\n",
    "perl \"$moses_scripts/tokenizer/detokenizer.perl\" -l en < \"$ref_dtc\" > \"$ref_detok\"\n",
    "\n",
    "score=$(sacrebleu \"$original_ref_file\" -i \"$hypo_detok\" -m bleu -b -w 4)\n",
    "\n",
    "# cleanup\n",
    "rm \"$src_tok\" \"$hypo_tok\" \"$ref_tok\" \"$src_dtc\" \"$hypo_dtc\" \"$ref_dtc\"\n",
    "echo \"Pipeline A (20k) BLEU SCORE: $score\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a744620b",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
